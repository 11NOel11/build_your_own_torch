# build_your_own_torch ğŸ”¥

**A ground-up, educational reimplementation of core PyTorch concepts** â€” autograd, tensors, neural network modules, and optimizers â€” built step by step from first principles.

**build_your_own_torch** is a learning-focused project whose goal is **understanding, not performance**.

Instead of treating deep learning frameworks as black boxes, this repo reconstructs their core ideas from scratch:

- **how automatic differentiation actually works**
- **how computation graphs are built and traversed**
- **how gradients flow via the chain rule**
- **how tensors, layers, and optimizers fit together**

If you've ever wondered what really happens when you call `loss.backward()` or `optimizer.step()`, this project is for you.

---

## âœ¨ Philosophy

Most deep learning libraries optimize for:
- speed
- hardware acceleration
- massive scale

**This project optimizes for:**
- **clarity**
- **minimalism**
- **correctness**
- **learning**

Everything is intentionally:
- **small**
- **explicit**
- **readable**
- **hackable**

The code is written to be **read, modified, and experimented with**.

---

## ğŸš€ Features (Current Status)

### âœ… Stage 1 â€” Scalar Autograd (Implemented)

- A `Value` class representing a scalar node in a computation graph
- Automatic construction of computation graphs through operator overloading
- Reverse-mode automatic differentiation (backpropagation)
- DFS-based topological sorting of computation graphs
- Local gradient rules via per-operation `_backward()` closures

**Supported operations:**
- Arithmetic: `+`, `-`, `*`, `/`, `**`
- Unary ops: negation
- Nonlinearities: `tanh`, `exp`

**Endâ€‘toâ€‘end training example:**
- Single tanh neuron
- Mean squared error loss
- Gradient descent loop
- Verified convergence

*This stage alone reproduces the core logic of PyTorch's autograd engine, but for scalars.*

---

## ğŸ§  Example: Scalar Autograd

```python
from build_your_own_torch.autograd_scalar import Value

x = Value(2.0)
y = Value(3.0)

f = x * y + x     # builds a computation graph
f.backward()      # runs backpropagation

print(f)          # Value(data=8.0, grad=1.0)
print(x.grad)     # 4.0
print(y.grad)     # 2.0
```

**Behind the scenes:**
- a computation graph is built automatically
- `.backward()` performs a reverse topological traversal
- each node applies its local chain-rule contribution

---

## ğŸ§ª Example: Training a Tiny Neuron

```python
from build_your_own_torch.autograd_scalar import Value

x1, x2 = Value(1.0), Value(-2.0)
w1, w2 = Value(0.5), Value(-1.0)
b = Value(0.0)

learning_rate = 0.1

for step in range(50):
    # forward pass
    n = (x1 * w1 + x2 * w2 + b).tanh()
    loss = (n - Value(0.0)) ** 2

    # zero gradients
    for p in (w1, w2, b):
        p.grad = 0.0

    # backward pass
    loss.backward()

    # gradient descent update
    for p in (w1, w2, b):
        p.data -= learning_rate * p.grad

    print(step, loss.data)
```

*This uses your own autograd engine, not PyTorch.*

---

## ğŸ—ºï¸ Roadmap

This project is built incrementally, with each stage layering new abstractions on top of the previous one.

### Stage 1 â€” Scalar Autograd âœ…
- âœ… `Value` class
- âœ… Computation graph construction
- âœ… Backpropagation with DFS + topological sort
- âœ… Scalar ops and nonlinearities
- âœ… Tiny neuron training example

### Stage 2 â€” Tensor Autograd (NumPy Backend)
- â¬œ `Tensor` class (`data: np.ndarray`, `grad: np.ndarray`)
- â¬œ Elementwise operations with broadcasting
- â¬œ Matrix multiplication
- â¬œ Reduction ops (sum, mean)
- â¬œ Reverse-mode autograd for tensors

### Stage 3 â€” Neural Network Modules
- â¬œ `Module` base class
- â¬œ Parameter registration
- â¬œ Layers: `Linear`, `ReLU`, `Tanh`
- â¬œ Loss functions (MSE, Crossâ€‘Entropy)

### Stage 4 â€” Optimizers
- â¬œ `Optimizer` base class
- â¬œ SGD
- â¬œ Momentum
- â¬œ RMSProp
- â¬œ Adam
- â¬œ AdamW
- â¬œ (Stretch) modern optimizers like Muon or Lion

### Stage 5 â€” Experiments & Examples
- â¬œ Linear regression
- â¬œ Multi-layer perceptron
- â¬œ Toy classification datasets
- â¬œ Optimizer comparisons

---

## ğŸ“¦ Installation (Using uv)

This project uses [**uv**](https://github.com/astral-sh/uv) for fast, reproducible Python environments.

### 1. Clone the repository

```bash
git clone https://github.com/11NOel11/build_your_own_torch.git
cd build_your_own_torch
```

### 2. Install uv (if needed)

```bash
pip install uv
```

### 3. Sync the environment

```bash
uv sync
```

This will:
- create an isolated virtual environment
- install dependencies defined in `pyproject.toml`

### 4. Run code inside the environment

```bash
uv run python examples/01_scalar_autograd_demo.py
```

---

## ğŸ“ Project Structure

```
build_your_own_torch/
â”‚
â”œâ”€â”€ build_your_own_torch/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ autograd_scalar.py     # Stage 1: scalar autograd
â”‚   â”œâ”€â”€ tensor.py              # Stage 2: tensor autograd (planned)
â”‚   â”œâ”€â”€ nn/                    # Stage 3: neural network modules
â”‚   â””â”€â”€ optim/                 # Stage 4: optimizers
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ 01_scalar_autograd_demo.py  # scalar neuron training demo
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ“Œ Status

This project is **actively evolving**.

Expect:
- refactors
- breaking changes
- improved abstractions
- deeper documentation as new stages are implemented

The commit history roughly follows the roadmap above and can be read as a **learning log**.

---

## ğŸ¤ Contributing

This is primarily a **personal learning project**, but contributions are welcome:

- open issues for conceptual discussions
- submit PRs for clarity improvements or bug fixes
- suggest extensions or experiments

If you're also building a framework from scratch, comparisons and discussions are encouraged.

---

## ğŸ“£ Build in Public

Progress on this project is documented daily as a **"build your own torch"** series on Twitter/X:

- design decisions
- small code snippets
- lessons learned

The goal is to make this repo useful not just as code, but as a **learning resource**.

---

## ğŸ“œ License

MIT License.
